# SparkBigData

### System general information

This project contains of 6 files with containers for each task and main.py which is used to run tests. Main program creates spark session and after that start running tests one by one. After that, results can be found in results directory. All containers were created to simplify data saving and for easier json transformation


### Code 
Code can be found in containers directory and in root (main.py)

### Results
Results can be found in results folder

### Link to S3
This part will be added soon. Because in this moment it is imposible for me to use AWS. And I need to use someone else cluster to run or find other way.

### Processing metrics
Everything what is written here was calculated for local spark server. 

| Test   |  Time  |
| :------|---------:|
| Test 1 | 13.38s   |
| Test 2 |11002.73s |
| Test 3 | 23.32s   |
| Test 4 |229.26s   |
| Test 5 |607.35s   |
| Test 6 | 46.28s   |


#### Size of input data 64.1 MB
#### Size of all output data 511.5 kB

#### Cluster params missed because of missing AWS

#### Execution parameters
![alt text](/analysis.png)
